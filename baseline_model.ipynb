{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model - Deep Neural Network Reinforcement Learning\n",
    "\n",
    "The baseline model will train an agent to manipulate two DMC's in series to maximize production. Instead of building a Q-table, the agent will learn a policy by approximating a reward function through neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from structure import DMC_structure\n",
    "from dmc import DMC_controller\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the setup for DMC's, which do XYZ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DMC array: [[0, [0, 1], 'DMC1', 400, [350, 5, 1]], [1, [2], 'DMC2', 500, [350, 5, 1]], [2, [], 'Dummy', 0, [0, 0, 0]]]\n"
     ]
    }
   ],
   "source": [
    "DMCarr = [[] for i in [0, 1]]\n",
    "            # index, next, fung, goal, input (T, P, Keq)\n",
    "DMCarr[0] = [0, [0, 1], \"DMC1\", 400, [350, 5, 1]]\n",
    "DMCarr[1] = [1, [], \"DMC2\", 500, [350, 5, 1]]\n",
    "# DMCarr[2] = [2, [], \"Dummy\", 0, [0, 0, 0]]\n",
    "\n",
    "print(\"DMC array:\", DMCarr)\n",
    "struct = DMC_structure(DMCarr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the reward function. Currently, it considers a positive reward based on the output of the system, and a negative reward based on violating the constraints of the system (i.e. manipulating the DMC's such that the system has a temperature of 500C when it should at most be 475C). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward function for reinforcement learning\n",
    "def reward_function(struct, output, valid_range=(-1, 1)):\n",
    "    \"\"\"\n",
    "    Computes the reward based on output and penalizes if DMC values are outside the valid range.\n",
    "    \"\"\"\n",
    "    p_reward_scale = 1.0 \n",
    "\n",
    "    p_reward = p_reward_scale * output # reward = m * output value (last value of last DMC)4\n",
    "    \n",
    "    n_reward = 0\n",
    "\n",
    "    #example: [[399.0709079721861, 400, 600], [2.112225163940035, 4.5, 6], [3.990709079721861, 4.2, inf]]\n",
    "    \n",
    "    #for each DMC\n",
    "    for i in range(len(DMCarr)):\n",
    "        #get all constraints of the DMC\n",
    "        for bound in struct.getConstraints(i):\n",
    "            #bound: [current, LB, UB]\n",
    "            buffer = (bound[2]-bound[1])*0.1\n",
    "            #if inf bounds, just have zero buffer - TODO: make this better\n",
    "            if buffer == float('inf'):\n",
    "                buffer = 0\n",
    "            UBB = bound[2] - buffer\n",
    "            LBB = bound[1] + buffer\n",
    "            if bound[0] > LBB and bound[0] < UBB:\n",
    "                nwd = 0\n",
    "            else:\n",
    "                #TODO: make a more sophisticated negative reward, i.e. exponential curve\n",
    "                nwd = 5\n",
    "\n",
    "            n_reward += nwd\n",
    "    \n",
    "    reward = p_reward - n_reward\n",
    "\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In supervised and unsupervised learning, we start with labeled or unlabeled data, and then train models based on this data. For reinforcement learning, we will generate an enviornment for the agent to explore. This simulated enviornment is then our training data, and our loss function is then based on our reward funciton.\n",
    "\n",
    "We will simulate this enviornment by having initial conditions that feed into a network of DMC's. For the baseline model, this will just be two DMC's in series. The final DMC in this simulated process will have some output, that we want to maximize.\n",
    "\n",
    "The reinforcement learning agent has the ability to \"explore\" the enviornment by setting different goals for the DMC's. The agent cannot fine tune the inner working of the DMC's themselves - these DMC's internally have non-linear processes (like PID) to reach the goal state from the current state.\n",
    "\n",
    "To use an analogy, the agent is the conductor, and the DMC's are musicians in the orchestra. The conductor cannot go and play the first violins themselves, but they can tell them what to play. The conductor needs to now explore and figure out how to make the best sound without breaking any instruments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate training data\n",
    "def generate_data(env_function, num_samples=500, valid_range=(-1, 1)):\n",
    "    \"\"\"\n",
    "    env_function: A function that takes in (DMC1, DMC2) and returns the output quantity.\n",
    "    num_samples: Number of data points to collect.\n",
    "    \"\"\"\n",
    "    dmc_values = np.random.uniform(low=valid_range[0], high=valid_range[1], size=(num_samples, 2))\n",
    "    outputs = np.array([env_function(dmc[0], dmc[1]) for dmc in dmc_values])\n",
    "    rewards = np.array([reward_function(dmc[0], dmc[1], output, valid_range) for dmc, output in zip(dmc_values, outputs)])\n",
    "    return dmc_values, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the neural network\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(16, activation='relu', input_shape=(2,)),  # First hidden layer\n",
    "    layers.Dense(16, activation='relu'),  # Second hidden layer\n",
    "    layers.Dense(1, activation='linear')  # Output layer (regression task)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss='mse')\n",
    "\n",
    "# Train the model\n",
    "def train_model(env_function, num_samples=500, epochs=100, valid_range=(-1, 1)):\n",
    "    X_train, y_train = generate_data(env_function, num_samples, valid_range)\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=32, verbose=1)\n",
    "    return model\n",
    "\n",
    "# Finding the optimal policy\n",
    "def find_optimal_dmc(valid_range=(-1, 1)):\n",
    "    \"\"\"\n",
    "    Optimizes DMC1 and DMC2 values to maximize the reward using the trained model.\n",
    "    \"\"\"\n",
    "    best_dmc = None\n",
    "    best_reward = -np.inf\n",
    "    \n",
    "    for _ in range(1000):  # Try 1000 random pairs\n",
    "        dmc_candidate = np.random.uniform(valid_range[0], valid_range[1], size=(1, 2))\n",
    "        predicted_reward = model.predict(dmc_candidate)[0, 0]\n",
    "        \n",
    "        if predicted_reward > best_reward:\n",
    "            best_reward = predicted_reward\n",
    "            best_dmc = dmc_candidate\n",
    "    \n",
    "    return best_dmc, best_reward\n",
    "\n",
    "# Example usage:\n",
    "# Define your environment function: env_function(DMC1, DMC2) -> output\n",
    "# Train the model and find optimal values\n",
    "# trained_model = train_model(env_function)\n",
    "# optimal_dmc, optimal_reward = find_optimal_dmc()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_proc-control",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
