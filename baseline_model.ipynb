{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model - Deep Neural Network Reinforcement Learning\n",
    "\n",
    "The baseline model will train an agent to manipulate two DMC's in series to maximize production. Instead of building a Q-table, the agent will learn a policy by approximating a reward function through neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DMCarr = [[] for i in [0, 1, 2]]\n",
    "            # index, next, fung, goal, input (T, P, Keq)\n",
    "DMCarr[0] = [0, [0, 1], \"DMC1\", 400, [350, 5, 1]]\n",
    "DMCarr[1] = [1, [2], \"DMC2\", 500, [350, 5, 1]]\n",
    "DMCarr[2] = [2, [], \"Dummy\", 0, [0, 0, 0]]\n",
    "\n",
    "print(\"DMC array:\", DMCarr)\n",
    "struct = DMC_structure(DMCarr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Define the neural network\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(16, activation='relu', input_shape=(2,)),  # First hidden layer\n",
    "    layers.Dense(16, activation='relu'),  # Second hidden layer\n",
    "    layers.Dense(1, activation='linear')  # Output layer (regression task)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss='mse')\n",
    "\n",
    "# Reward function for reinforcement learning\n",
    "def reward_function(struct, output, valid_range=(-1, 1)):\n",
    "    \"\"\"\n",
    "    Computes the reward based on output and penalizes if DMC values are outside the valid range.\n",
    "    \"\"\"\n",
    "    p_reward_scale = 1.0 \n",
    "\n",
    "    p_reward = p_reward_scale * output # reward = m * output value (last value of last DMC)4\n",
    "    \n",
    "    n_reward = 0\n",
    "    \n",
    "    for dmc in struct():\n",
    "        for bound in dmc.getConstraints()\n",
    "            buffer = (bound[2]-bound[1])*0.1\n",
    "            UBB = bound[2] - buffer\n",
    "            LBB = bound[1] + buffer\n",
    "            if bound[0] > LBB or bound[0] < UBB:\n",
    "                nwd = 0\n",
    "            else:\n",
    "\n",
    "    n_reward = struct.getConstraints(\n",
    "    if not (valid_range[0] <= dmc1 <= valid_range[1]):\n",
    "        reward -= 100  # Heavy penalty if DMC1 is out of range\n",
    "    if not (valid_range[0] <= dmc2 <= valid_range[1]):\n",
    "        reward -= 100  # Heavy penalty if DMC2 is out of range\n",
    "    \n",
    "    reward = p_reward - n_reward\n",
    "\n",
    "    return reward\n",
    "\n",
    "# Function to generate training data\n",
    "def generate_data(env_function, num_samples=500, valid_range=(-1, 1)):\n",
    "    \"\"\"\n",
    "    env_function: A function that takes in (DMC1, DMC2) and returns the output quantity.\n",
    "    num_samples: Number of data points to collect.\n",
    "    \"\"\"\n",
    "    dmc_values = np.random.uniform(low=valid_range[0], high=valid_range[1], size=(num_samples, 2))\n",
    "    outputs = np.array([env_function(dmc[0], dmc[1]) for dmc in dmc_values])\n",
    "    rewards = np.array([reward_function(dmc[0], dmc[1], output, valid_range) for dmc, output in zip(dmc_values, outputs)])\n",
    "    return dmc_values, rewards\n",
    "\n",
    "# Train the model\n",
    "def train_model(env_function, num_samples=500, epochs=100, valid_range=(-1, 1)):\n",
    "    X_train, y_train = generate_data(env_function, num_samples, valid_range)\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=32, verbose=1)\n",
    "    return model\n",
    "\n",
    "# Finding the optimal policy\n",
    "def find_optimal_dmc(valid_range=(-1, 1)):\n",
    "    \"\"\"\n",
    "    Optimizes DMC1 and DMC2 values to maximize the reward using the trained model.\n",
    "    \"\"\"\n",
    "    best_dmc = None\n",
    "    best_reward = -np.inf\n",
    "    \n",
    "    for _ in range(1000):  # Try 1000 random pairs\n",
    "        dmc_candidate = np.random.uniform(valid_range[0], valid_range[1], size=(1, 2))\n",
    "        predicted_reward = model.predict(dmc_candidate)[0, 0]\n",
    "        \n",
    "        if predicted_reward > best_reward:\n",
    "            best_reward = predicted_reward\n",
    "            best_dmc = dmc_candidate\n",
    "    \n",
    "    return best_dmc, best_reward\n",
    "\n",
    "# Example usage:\n",
    "# Define your environment function: env_function(DMC1, DMC2) -> output\n",
    "# Train the model and find optimal values\n",
    "# trained_model = train_model(env_function)\n",
    "# optimal_dmc, optimal_reward = find_optimal_dmc()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
