{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model - SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.0\n",
      "tf.keras available: True\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"tf.keras available:\", hasattr(tf, \"keras\"))\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from structure import DMC_structure\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "\n",
    "from DMC_Env import DMC_Env\n",
    "\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "from sac import SoftActorCritic, Actor\n",
    "from replay_buffer import ReplayBuffer\n",
    "\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "\n",
    "logging.basicConfig(level='INFO')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per the format explained above, the DMC chain is initialized below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DMC array: [[0, [0, 1], 'DMC0', 400, [350, 5, 1]], [1, [2], 'DMC1', 500, [350, 5, 1]], [2, [3, 5], 'DMC2', 500, [350, 5, 1]], [3, [4], 'DMC3', 500, [350, 5, 1]], [4, [5], 'DMC4', 500, [350, 5, 1]], [5, [], 'DMC5', 500, [350, 5, 1]], [6, [1, 6], 'DMC6', 500, [350, 5, 1]], [7, [3, 7], 'DMC7', 500, [350, 5, 1]], [8, [5, 8], 'DMC8', 500, [350, 5, 1]], [9, [2, 3, 4, 9], 'DMC9', 500, [350, 5, 1]]]\n"
     ]
    }
   ],
   "source": [
    "DMCarr = [[] for i in range(10)]\n",
    "            # index, next, func, goal, input (T, P, Keq)\n",
    "DMCarr[0] = [0, [0, 1], \"DMC0\", 400, [350, 5, 1]]\n",
    "DMCarr[1] = [1, [2], \"DMC1\", 500, [350, 5, 1]]\n",
    "DMCarr[2] = [2, [3, 5], \"DMC2\", 500, [350, 5, 1]]\n",
    "DMCarr[3] = [3, [4], \"DMC3\", 500, [350, 5, 1]]\n",
    "DMCarr[4] = [4, [5], \"DMC4\", 500, [350, 5, 1]]\n",
    "DMCarr[5] = [5, [], \"DMC5\", 500, [350, 5, 1]]\n",
    "DMCarr[6] = [6, [1, 6], \"DMC6\", 500, [350, 5, 1]]\n",
    "DMCarr[7] = [7, [3, 7], \"DMC7\", 500, [350, 5, 1]]\n",
    "DMCarr[8] = [8, [5, 8], \"DMC8\", 500, [350, 5, 1]]\n",
    "DMCarr[9] = [9, [2, 3, 4, 9], \"DMC9\", 500, [350, 5, 1]]\n",
    "# DMCarr[2] = [2, [], \"Dummy\", 0, [0, 0, 0]]\n",
    "\n",
    "print(\"DMC array:\", DMCarr)\n",
    "struct = DMC_structure(DMCarr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enviornment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'seed': 42,\n",
    "    'render': False,\n",
    "    'verbose': False,\n",
    "    'batch_size': 128,\n",
    "    'epochs': 50,\n",
    "    'start_steps': 0,\n",
    "    'model_path': '../data/models/',\n",
    "    'model_name': f'{str(datetime.utcnow().date())}-{str(datetime.utcnow().time())}',\n",
    "    'gamma': 0.99,\n",
    "    'polyak': 0.995,\n",
    "    'learning_rate': 0.001,\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DMC environment setup\n",
    "env = DMC_Env(DMCarr)\n",
    "\n",
    "state_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.shape[0]\n",
    "\n",
    "replay = ReplayBuffer(state_space, action_space)\n",
    "\n",
    "log_dir = args['model_path'] + '/logs/' + datetime.utcnow().strftime(\"%Y%m%d-%H%M%S\")\n",
    "writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "sac = SoftActorCritic(action_space, writer,\n",
    "                      learning_rate=args['learning_rate'],\n",
    "                      gamma=args['gamma'],\n",
    "                      polyak=args['polyak'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10 #of episodes to run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 reward: -597.085516909237\n",
      "Episode 1 Average episode reward: -597.085516909237\n",
      "Episode 2 reward: -393.5479818963556\n",
      "Episode 2 Average episode reward: -495.3167494027963\n",
      "Change in average reward: 101.76876750644067\n",
      "Episode 3 reward: -448.81738717805666\n",
      "Episode 3 Average episode reward: -479.81696199454973\n",
      "Change in average reward: 15.499787408246561\n",
      "Episode 4 reward: -369.0162820462625\n",
      "Episode 4 Average episode reward: -452.1167920074779\n",
      "Change in average reward: 27.70016998707183\n",
      "Episode 5 reward: -339.9266594638846\n",
      "Episode 5 Average episode reward: -429.6787654987593\n",
      "Change in average reward: 22.43802650871862\n",
      "Episode 6 reward: -432.84519436106046\n",
      "Episode 6 Average episode reward: -430.2065036424761\n",
      "Change in average reward: -0.5277381437168174\n",
      "Episode 7 reward: -444.0187585979946\n",
      "Episode 7 Average episode reward: -432.17968292183593\n",
      "Change in average reward: -1.9731792793598402\n",
      "Episode 8 reward: -428.936617018208\n",
      "Episode 8 Average episode reward: -431.7742996838824\n",
      "Change in average reward: 0.40538323795351516\n",
      "Episode 9 reward: -396.48056607986047\n",
      "Episode 9 Average episode reward: -427.85277372788\n",
      "Change in average reward: 3.9215259560024265\n",
      "Episode 10 reward: -427.6637492123328\n",
      "Episode 10 Average episode reward: -427.83387127632534\n",
      "Change in average reward: 0.01890245155465209\n"
     ]
    }
   ],
   "source": [
    "episode_rewards = []\n",
    "global_step = 0\n",
    "episode = 0\n",
    "prev_avg_episode_reward = None\n",
    "\n",
    "# Run for n episodes\n",
    "for _ in range(n):\n",
    "    current_state = env.reset()\n",
    "    step = 1\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        if args['render']:\n",
    "            env.render()\n",
    "\n",
    "        # Choose action: use fully random action during exploration phase.\n",
    "        if global_step < args['start_steps']:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = sac.sample_action(current_state)\n",
    "            # In case the action is still a scalar, force it into an array.\n",
    "            if np.isscalar(action):\n",
    "                action = np.array([action])\n",
    "\n",
    "        # Step in the environment.\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        end = 0 if done else 1\n",
    "\n",
    "        # Optional logging per step.\n",
    "        if args.get('verbose', False):\n",
    "            logging.info(f\"Global step: {global_step}\")\n",
    "            logging.info(f\"Current state: {current_state}\")\n",
    "            logging.info(f\"Action: {action}\")\n",
    "            logging.info(f\"Reward: {reward}\")\n",
    "            logging.info(f\"Next state: {next_state}\")\n",
    "            logging.info(f\"End flag: {end}\")\n",
    "\n",
    "        # Store the transition in replay buffer.\n",
    "        replay.store(current_state, action, reward, next_state, end)\n",
    "\n",
    "        current_state = next_state\n",
    "        step += 1\n",
    "        global_step += 1\n",
    "\n",
    "    # Training: only if enough samples are available and the exploration phase is over.\n",
    "    if replay.total_size > args['batch_size'] and global_step > args['start_steps']:\n",
    "        for epoch in range(args['epochs']):\n",
    "            current_states, actions, rewards, next_states, ends = replay.fetch_sample(num_samples=args['batch_size'])\n",
    "            critic1_loss, critic2_loss, actor_loss, alpha_loss = sac.train(\n",
    "                current_states, actions, rewards, next_states, ends\n",
    "            )\n",
    "\n",
    "            if args.get('verbose', False):\n",
    "                print(f\"Episode {episode}, Global step {global_step}, Epoch {epoch}:\",\n",
    "                      critic1_loss.numpy(), critic2_loss.numpy(),\n",
    "                      actor_loss.numpy(), f\"Episode Reward: {episode_reward}\")\n",
    "\n",
    "            # Increase the training epoch step and update target networks each epoch.\n",
    "            sac.epoch_step += 1\n",
    "            sac.update_weights()  # Now uses assign() inside the SAC module.\n",
    "\n",
    "    # Save model every 100 episodes (adjustable as needed).\n",
    "    if episode % 1 == 0:\n",
    "        # Generate a safe timestamp without invalid characters (no colons)\n",
    "        safe_timestamp = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "        args['model_name'] = safe_timestamp\n",
    "\n",
    "        # Define the full directory where the model will be saved\n",
    "        model_dir = args['model_path'] + args['model_name']\n",
    "\n",
    "        # Create the directory if it doesn't exist\n",
    "        if not os.path.exists(model_dir):\n",
    "            os.makedirs(model_dir)\n",
    "\n",
    "        # Save model weights using the corrected file path\n",
    "        sac.policy.save_weights(model_dir + '/model.weights.h5')\n",
    "\n",
    "    # Update reward history and compute the average over the last 100 episodes.\n",
    "    episode_rewards.append(episode_reward)\n",
    "    episode += 1\n",
    "    avg_episode_reward = sum(episode_rewards[-100:]) / len(episode_rewards[-100:])\n",
    "\n",
    "    # Print the reward and average.\n",
    "    print(f\"Episode {episode} reward: {episode_reward}\")\n",
    "    print(f\"Episode {episode} Average episode reward: {avg_episode_reward}\")\n",
    "\n",
    "    # Calculate and print the change in average reward compared to the previous episode.\n",
    "    if prev_avg_episode_reward is not None:\n",
    "        change = avg_episode_reward - prev_avg_episode_reward\n",
    "        print(f\"Change in average reward: {change}\")\n",
    "    prev_avg_episode_reward = avg_episode_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after training:\n",
    "save_dir = os.path.join(args['model_path'], args['model_name'])\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "sac.actor.save_weights(os.path.join(save_dir, \"model\"))  # ← no extension\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do\n",
    "\n",
    "- how do I actually extract the final correct policy?\n",
    "\n",
    "Done:\n",
    "- running on GPU, but push to ICE in worst case. Every loop now takes just 30 seconds.\n",
    "- Reward function is updated, with minimal performance improvements. It's still absurdly negative - which indicates that the actions picked and initial conditions are terrible.\n",
    "- testing script shows that the action space is correctly defined - then why is reward so bad? \n",
    "\n",
    "SAC3:\n",
    "- now fixed the scaling in the action space tanh\n",
    "- fixed the scaling in the reward function to not explode\n",
    "- the reason why we have average ereward and episode reward is becuase we keep a running average to track how we are doing over time. we do see an update, and we do also see a convergence to a stable solution within 10 episodes. \n",
    "- We should figure out a way to print the final model and that's it - recommend a stateless bandit.\n",
    "\n",
    "Discussion:\n",
    "- well.. it does converge even if it's terrible. \n",
    "- stateless bandit as a \"testing version\"\n",
    "- it's in the DMCs.. actually aligned with baseline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_tf210",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
