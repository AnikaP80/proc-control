{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model - SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.0\n",
      "tf.keras available: True\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"tf.keras available:\", hasattr(tf, \"keras\"))\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from structure import DMC_structure\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "\n",
    "from DMC_Env import DMC_Env\n",
    "\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "from sac import SoftActorCritic\n",
    "from replay_buffer import ReplayBuffer\n",
    "\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "\n",
    "logging.basicConfig(level='INFO')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per the format explained above, the DMC chain is initialized below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DMC array: [[0, [0, 1], 'DMC0', 400, [350, 5, 1]], [1, [2], 'DMC1', 500, [350, 5, 1]], [2, [3, 5], 'DMC2', 500, [350, 5, 1]], [3, [4], 'DMC3', 500, [350, 5, 1]], [4, [5], 'DMC4', 500, [350, 5, 1]], [5, [], 'DMC5', 500, [350, 5, 1]], [6, [1, 6], 'DMC6', 500, [350, 5, 1]], [7, [3, 7], 'DMC7', 500, [350, 5, 1]], [8, [5, 8], 'DMC8', 500, [350, 5, 1]], [9, [2, 3, 4, 9], 'DMC9', 500, [350, 5, 1]]]\n"
     ]
    }
   ],
   "source": [
    "DMCarr = [[] for i in range(10)]\n",
    "            # index, next, func, goal, input (T, P, Keq)\n",
    "DMCarr[0] = [0, [0, 1], \"DMC0\", 400, [350, 5, 1]]\n",
    "DMCarr[1] = [1, [2], \"DMC1\", 500, [350, 5, 1]]\n",
    "DMCarr[2] = [2, [3, 5], \"DMC2\", 500, [350, 5, 1]]\n",
    "DMCarr[3] = [3, [4], \"DMC3\", 500, [350, 5, 1]]\n",
    "DMCarr[4] = [4, [5], \"DMC4\", 500, [350, 5, 1]]\n",
    "DMCarr[5] = [5, [], \"DMC5\", 500, [350, 5, 1]]\n",
    "DMCarr[6] = [6, [1, 6], \"DMC6\", 500, [350, 5, 1]]\n",
    "DMCarr[7] = [7, [3, 7], \"DMC7\", 500, [350, 5, 1]]\n",
    "DMCarr[8] = [8, [5, 8], \"DMC8\", 500, [350, 5, 1]]\n",
    "DMCarr[9] = [9, [2, 3, 4, 9], \"DMC9\", 500, [350, 5, 1]]\n",
    "# DMCarr[2] = [2, [], \"Dummy\", 0, [0, 0, 0]]\n",
    "\n",
    "print(\"DMC array:\", DMCarr)\n",
    "struct = DMC_structure(DMCarr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enviornment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'seed': 42,\n",
    "    'render': False,\n",
    "    'verbose': False,\n",
    "    'batch_size': 128,\n",
    "    'epochs': 50,\n",
    "    'start_steps': 0,\n",
    "    'model_path': '../data/models/',\n",
    "    'model_name': f'{str(datetime.utcnow().date())}-{str(datetime.utcnow().time())}',\n",
    "    'gamma': 0.99,\n",
    "    'polyak': 0.995,\n",
    "    'learning_rate': 0.001,\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DMC environment setup\n",
    "env = DMC_Env(DMCarr)\n",
    "\n",
    "state_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.shape[0]\n",
    "\n",
    "replay = ReplayBuffer(state_space, action_space)\n",
    "\n",
    "log_dir = args['model_path'] + '/logs/' + datetime.utcnow().strftime(\"%Y%m%d-%H%M%S\")\n",
    "writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "sac = SoftActorCritic(action_space, writer,\n",
    "                      learning_rate=args['learning_rate'],\n",
    "                      gamma=args['gamma'],\n",
    "                      polyak=args['polyak'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10 #of episodes to run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 reward: -198602.1341365208\n",
      "Episode 1 Average episode reward: -198602.1341365208\n",
      "Episode 2 reward: nan\n",
      "Episode 2 Average episode reward: nan\n",
      "Change in average reward: nan\n",
      "Episode 3 reward: nan\n",
      "Episode 3 Average episode reward: nan\n",
      "Change in average reward: nan\n",
      "Episode 4 reward: nan\n",
      "Episode 4 Average episode reward: nan\n",
      "Change in average reward: nan\n",
      "Episode 5 reward: nan\n",
      "Episode 5 Average episode reward: nan\n",
      "Change in average reward: nan\n",
      "Episode 6 reward: nan\n",
      "Episode 6 Average episode reward: nan\n",
      "Change in average reward: nan\n",
      "Episode 7 reward: nan\n",
      "Episode 7 Average episode reward: nan\n",
      "Change in average reward: nan\n",
      "Episode 8 reward: nan\n",
      "Episode 8 Average episode reward: nan\n",
      "Change in average reward: nan\n",
      "Episode 9 reward: nan\n",
      "Episode 9 Average episode reward: nan\n",
      "Change in average reward: nan\n",
      "Episode 10 reward: nan\n",
      "Episode 10 Average episode reward: nan\n",
      "Change in average reward: nan\n"
     ]
    }
   ],
   "source": [
    "episode_rewards = []\n",
    "global_step = 0\n",
    "episode = 0\n",
    "prev_avg_episode_reward = None\n",
    "\n",
    "# Run for n episodes\n",
    "for _ in range(n):\n",
    "    current_state = env.reset()\n",
    "    step = 1\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        if args['render']:\n",
    "            env.render()\n",
    "\n",
    "        # Choose action: use fully random action during exploration phase.\n",
    "        if global_step < args['start_steps']:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = sac.sample_action(current_state)\n",
    "            # In case the action is still a scalar, force it into an array.\n",
    "            if np.isscalar(action):\n",
    "                action = np.array([action])\n",
    "\n",
    "        # Step in the environment.\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        end = 0 if done else 1\n",
    "\n",
    "        # Optional logging per step.\n",
    "        if args.get('verbose', False):\n",
    "            logging.info(f\"Global step: {global_step}\")\n",
    "            logging.info(f\"Current state: {current_state}\")\n",
    "            logging.info(f\"Action: {action}\")\n",
    "            logging.info(f\"Reward: {reward}\")\n",
    "            logging.info(f\"Next state: {next_state}\")\n",
    "            logging.info(f\"End flag: {end}\")\n",
    "\n",
    "        # Store the transition in replay buffer.\n",
    "        replay.store(current_state, action, reward, next_state, end)\n",
    "\n",
    "        current_state = next_state\n",
    "        step += 1\n",
    "        global_step += 1\n",
    "\n",
    "    # Training: only if enough samples are available and the exploration phase is over.\n",
    "    if replay.total_size > args['batch_size'] and global_step > args['start_steps']:\n",
    "        for epoch in range(args['epochs']):\n",
    "            current_states, actions, rewards, next_states, ends = replay.fetch_sample(num_samples=args['batch_size'])\n",
    "            critic1_loss, critic2_loss, actor_loss, alpha_loss = sac.train(\n",
    "                current_states, actions, rewards, next_states, ends\n",
    "            )\n",
    "\n",
    "            if args.get('verbose', False):\n",
    "                print(f\"Episode {episode}, Global step {global_step}, Epoch {epoch}:\",\n",
    "                      critic1_loss.numpy(), critic2_loss.numpy(),\n",
    "                      actor_loss.numpy(), f\"Episode Reward: {episode_reward}\")\n",
    "\n",
    "            # Increase the training epoch step and update target networks each epoch.\n",
    "            sac.epoch_step += 1\n",
    "            sac.update_weights()  # Now uses assign() inside the SAC module.\n",
    "\n",
    "    # Save model every 100 episodes (adjustable as needed).\n",
    "    if episode % 1 == 0:\n",
    "        # Generate a safe timestamp without invalid characters (no colons)\n",
    "        safe_timestamp = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "        args['model_name'] = safe_timestamp\n",
    "\n",
    "        # Define the full directory where the model will be saved\n",
    "        model_dir = args['model_path'] + args['model_name']\n",
    "\n",
    "        # Create the directory if it doesn't exist\n",
    "        if not os.path.exists(model_dir):\n",
    "            os.makedirs(model_dir)\n",
    "\n",
    "        # Save model weights using the corrected file path\n",
    "        sac.policy.save_weights(model_dir + '/model.weights.h5')\n",
    "\n",
    "    # Update reward history and compute the average over the last 100 episodes.\n",
    "    episode_rewards.append(episode_reward)\n",
    "    episode += 1\n",
    "    avg_episode_reward = sum(episode_rewards[-100:]) / len(episode_rewards[-100:])\n",
    "\n",
    "    # Print the reward and average.\n",
    "    print(f\"Episode {episode} reward: {episode_reward}\")\n",
    "    print(f\"Episode {episode} Average episode reward: {avg_episode_reward}\")\n",
    "\n",
    "    # Calculate and print the change in average reward compared to the previous episode.\n",
    "    if prev_avg_episode_reward is not None:\n",
    "        change = avg_episode_reward - prev_avg_episode_reward\n",
    "        print(f\"Change in average reward: {change}\")\n",
    "    prev_avg_episode_reward = avg_episode_reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do\n",
    "\n",
    "- why is avg = reward?\n",
    "- why is absurdly negative? fix reward\n",
    "- run on GPU - why taking so long? rewrite packages for my PC, ice, or anika PC\n",
    "- how do I actually extract the final correct policy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "The program predicted that the optimal DMC set temperatures should be 897 degrees and 995 degrees. We have found that this solution is not stable, and that previous iterations have predicted other temperatures such as 500 degrees and 998 degrees. Given that our bounds are (250, 500) and (400, 600), the reward function needs more tuning in order to stay within the bounds of our system and provide a stable solution. We tried different combinations of functions, and did not get a satisfactory result - we may write a continous rather than a linear reward function to address this.\n",
    "\n",
    "In regards to the training, the final MSE and MAE values are relatively low and stable, indicating that the model has learned a reasonable mapping from goals (inputs) to rewards (outputs).\n",
    "The small gap between training and validation suggests that the model is generalizing well rather than just memorizing the training data. Monitoring both MSE and MAE provides a more complete picture of performance: MSE helps drive the training optimization, and MAE offers a straightforward “average error” interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary:\n",
    "\n",
    "1. The goal is to have an agent control multiple DMC's within a complex process to maximize some value through reinforcement learning\n",
    "2. Each DMC takes in some start conditions and attempts to optimize them within their own controls, like a PID controller for temperature. The agent only changes the goal, not the behavior itself.\n",
    "3. The agent wants to maximize a reward, which is a balance between increasing $K_{eq}$ as much as possible, and staying within constraints.\n",
    "\n",
    "For the baseline model:\n",
    "1. The process is two DMC's in series.\n",
    "2. The reward is based on a linear positive reward for $K_{eq}$ and a fixed negative reward for each violation of bounds\n",
    "3. The exploration is a naive approach of generating data at random points, from fixed start conditions.\n",
    "4. Due to the naive approach, the training basically becomes supervised learning of finding rewards given our exploration batch.\n",
    "5. Due to the naive approach, finding the optimal value becomes a search through the supervised model.\n",
    "\n",
    "We found:\n",
    "1. The model trained and generalized well\n",
    "2. We need to adjust our reward function\n",
    "\n",
    "In the future we hope to:\n",
    "1. Optimize a far more complex process, with many more DMC's in parallel and in series, with more random fluctuations.\n",
    "2. Have a more sophisiticated and balanced reward function.\n",
    "3. Do an exploration based on seeking a better reward rather than the naive approach.\n",
    "4. Transition from Naive learning to Q-learning, then to more advanced RL algorithms.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_tf210",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
